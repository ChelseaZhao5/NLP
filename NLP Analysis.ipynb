{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Analysis via the ML-based approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import textstat\n",
    "import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                    Sentence  Polarity\n",
      "0                   Wow... Loved this place.         1\n",
      "1                         Crust is not good.         0\n",
      "2  Not tasty and the texture was just nasty.         0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                    Sentence  Polarity\n",
      "0                   Wow... Loved this place.         1\n",
      "1                         Crust is not good.         0\n",
      "2  Not tasty and the texture was just nasty.         0\n"
     ]
    }
   ],
   "source": [
    "#importing the training and test datasetses\n",
    "#check the number of instances in each dataset and check if there are missing valu\n",
    "\n",
    "df_train = pd.read_csv(\"sentiment_train.csv\")\n",
    "print(df_train.info())\n",
    "print(df_train.head(3))\n",
    "\n",
    "df_test = pd.read_csv(\"sentiment_test.csv\")\n",
    "print(df_train.info())\n",
    "print(df_train.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Polarity', ylabel='count'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ40lEQVR4nO3de4yldX3H8fdHVrxVBWFCcZd2Sd3aUGu9bJBo2hCxinhZa9VgvKxIujXBe1NFmxRqY6LRShGNzQZQ1lgUr6yG1iJqvVTQQQlyqXGCF3aDMgqi9YZrv/3j/DYc19n9nZ2dc84s5/1KTuZ5vr/f85zvJJP55Lmc56SqkCRpX+4x7QYkSaufYSFJ6jIsJEldhoUkqcuwkCR1rZl2A+Nw5JFH1vr166fdhiQdVK6++uofVNXcUmN3y7BYv3498/Pz025Dkg4qSb6ztzFPQ0mSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUNbawSHJhkluTXDdUe0uS/0lybZKPJjlsaOx1SRaSfCPJk4bqJ7faQpIzx9WvJGnvxvkJ7vcA7wC2DdUuB15XVbuSvBl4HfDaJMcBpwJ/DDwY+FSSP2zbvBP4C2AH8JUk26vqhjH2DcCj/25bf5JmztVveeG0W5CmYmxHFlX1OeC2PWr/WVW72uqVwLq2vAl4f1X9sqq+BSwAx7fXQlXdVFV3Au9vcyVJEzTNaxYvBv69La8Fbh4a29Fqe6v/liRbkswnmV9cXBxDu5I0u6YSFkn+HtgFvG+l9llVW6tqY1VtnJtb8qGJkqRlmvhTZ5O8CHgqcFJVVSvvBI4Zmrau1dhHXZpZ333Dn0y7Ba1Cv/cPXx/bvid6ZJHkZOA1wNOr6mdDQ9uBU5PcK8mxwAbgy8BXgA1Jjk1yKIOL4Nsn2bMkaYxHFkkuBk4EjkyyAziLwd1P9wIuTwJwZVW9pKquT3IJcAOD01NnVNWv235eCnwSOAS4sKquH1fPkqSljS0squq5S5Qv2Mf8NwJvXKJ+GXDZCrYmSdpPfoJbktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktQ1trBIcmGSW5NcN1R7UJLLk3yz/Ty81ZPk7UkWklyb5FFD22xu87+ZZPO4+pUk7d04jyzeA5y8R+1M4Iqq2gBc0dYBngxsaK8twLtgEC7AWcBjgOOBs3YHjCRpcsYWFlX1OeC2PcqbgIva8kXAM4bq22rgSuCwJEcDTwIur6rbqup24HJ+O4AkSWM26WsWR1XVLW35e8BRbXktcPPQvB2ttrf6b0myJcl8kvnFxcWV7VqSZtzULnBXVQG1gvvbWlUbq2rj3NzcSu1WksTkw+L77fQS7eetrb4TOGZo3rpW21tdkjRBkw6L7cDuO5o2A5cO1V/Y7oo6Abijna76JPDEJIe3C9tPbDVJ0gStGdeOk1wMnAgcmWQHg7ua3gRckuR04DvAc9r0y4BTgAXgZ8BpAFV1W5J/Ar7S5r2hqva8aC5JGrOxhUVVPXcvQyctMbeAM/aynwuBC1ewNUnSfvIT3JKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkrqmERZJXJbk+yXVJLk5y7yTHJrkqyUKSDyQ5tM29V1tfaOPrp9GzJM2yiYdFkrXAy4GNVfUw4BDgVODNwDlV9RDgduD0tsnpwO2tfk6bJ0maoGmdhloD3CfJGuC+wC3A44EPtfGLgGe05U1tnTZ+UpJMrlVJ0sTDoqp2Am8FvssgJO4ArgZ+VFW72rQdwNq2vBa4uW27q80/Ys/9JtmSZD7J/OLi4nh/CUmaMdM4DXU4g6OFY4EHA/cDTj7Q/VbV1qraWFUb5+bmDnR3kqQh0zgN9QTgW1W1WFW/Aj4CPA44rJ2WAlgH7GzLO4FjANr4A4EfTrZlSZpt0wiL7wInJLlvu/ZwEnAD8BngWW3OZuDStry9rdPGP11VNcF+JWnmTeOaxVUMLlR/Ffh662Er8Frg1UkWGFyTuKBtcgFwRKu/Gjhz0j1L0qxb05+y8qrqLOCsPco3AccvMfcXwLMn0ZckaWl+gluS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVLXSGGR5IpRapKku6d9Pkgwyb0ZfO3pke1Li3Z/nekDuOub7CRJd3O9p87+DfBKBt9odzV3hcWPgXeMry1J0mqyz7CoqnOBc5O8rKrOm1BPkqRVZqTvs6iq85I8Flg/vE1VbRtTX5KkVWSksEjyXuAPgGuAX7dyAYaFJM2AUb8pbyNwnN99LUmzadTPWVwH/O44G5EkrV6jHlkcCdyQ5MvAL3cXq+rpY+lKkrSqjBoWZ4+zCUnS6jbq3VD/Ne5GJEmr16h3Q/2Ewd1PAIcC9wR+WlUPGFdjkqTVY9Qji/vvXk4SYBNwwriakiStLvv91Nka+BjwpJVvR5K0Go16GuqZQ6v3YPC5i18s902THAacDzyMwemtFwPfAD7A4FPi3waeU1W3tyOZc4FTgJ8BL6qqry73vSVJ+2/UI4unDb2eBPyEwamo5ToX+I+q+iPgT4EbgTOBK6pqA3BFWwd4MrChvbYA7zqA95UkLcOo1yxOW6k3TPJA4M+BF7V93wncmWQTcGKbdhHwWeC1DEJpW/v0+JVJDktydFXdslI9SZL2bdQvP1qX5KNJbm2vDydZt8z3PBZYBN6d5GtJzk9yP+CooQD4HnBUW14L3Dy0/Q6W+C6NJFuSzCeZX1xcXGZrkqSljHoa6t3Adgbfa/Fg4OOtthxrgEcB76qqRwI/5a5TTsDgIjp33ao7kqraWlUbq2rj3NzcMluTJC1l1LCYq6p3V9Wu9noPsNz/yDuAHVV1VVv/EIPw+H6SowHaz1vb+E7gmKHt17WaJGlCRg2LHyZ5fpJD2uv5wA+X84ZV9T3g5iQPbaWTgBsYHLlsbrXNwKVteTvwwgycANzh9QpJmqxRnw31YuA84BwGp4f+m3aBepleBrwvyaHATcBpDILrkiSnA98BntPmXsbgttkFBrfOrtjFdknSaEYNizcAm6vqdoAkDwLeyiBE9ltVXcPgsxp7OmmJuQWcsZz3kSStjFFPQz18d1AAVNVtwCPH05IkabUZNSzukeTw3SvtyGLUoxJJ0kFu1H/4/wx8KckH2/qzgTeOpyVJ0moz6ie4tyWZBx7fSs+sqhvG15YkaTUZ+VRSCwcDQpJm0H4/olySNHsMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldUwuLJIck+VqST7T1Y5NclWQhyQeSHNrq92rrC218/bR6lqRZNc0ji1cANw6tvxk4p6oeAtwOnN7qpwO3t/o5bZ4kaYKmEhZJ1gFPAc5v6wEeD3yoTbkIeEZb3tTWaeMntfmSpAmZ1pHFvwCvAf6vrR8B/KiqdrX1HcDatrwWuBmgjd/R5kuSJmTiYZHkqcCtVXX1Cu93S5L5JPOLi4sruWtJmnnTOLJ4HPD0JN8G3s/g9NO5wGFJ1rQ564CdbXkncAxAG38g8MM9d1pVW6tqY1VtnJubG+9vIEkzZuJhUVWvq6p1VbUeOBX4dFU9D/gM8Kw2bTNwaVve3tZp45+uqppgy5I081bT5yxeC7w6yQKDaxIXtPoFwBGt/mrgzCn1J0kza01/yvhU1WeBz7blm4Djl5jzC+DZE21MkvQbVtORhSRplTIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6pp4WCQ5JslnktyQ5Pokr2j1ByW5PMk328/DWz1J3p5kIcm1SR416Z4ladZN48hiF/C3VXUccAJwRpLjgDOBK6pqA3BFWwd4MrChvbYA75p8y5I02yYeFlV1S1V9tS3/BLgRWAtsAi5q0y4CntGWNwHbauBK4LAkR0+2a0mabVO9ZpFkPfBI4CrgqKq6pQ19DziqLa8Fbh7abEerSZImZGphkeR3gA8Dr6yqHw+PVVUBtZ/725JkPsn84uLiCnYqSZpKWCS5J4OgeF9VfaSVv7/79FL7eWur7wSOGdp8Xav9hqraWlUbq2rj3Nzc+JqXpBk0jbuhAlwA3FhVbxsa2g5sbsubgUuH6i9sd0WdANwxdLpKkjQBa6bwno8DXgB8Pck1rfZ64E3AJUlOB74DPKeNXQacAiwAPwNOm2i3kqTJh0VVfQHIXoZPWmJ+AWeMtSlJ0j75CW5JUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqOmjCIsnJSb6RZCHJmdPuR5JmyUERFkkOAd4JPBk4DnhukuOm25UkzY6DIiyA44GFqrqpqu4E3g9smnJPkjQz1ky7gRGtBW4eWt8BPGZ4QpItwJa2+r9JvjGh3mbBkcAPpt3EapC3bp52C/pt/n3udlYOdA+/v7eBgyUsuqpqK7B12n3cHSWZr6qN0+5DWop/n5NxsJyG2gkcM7S+rtUkSRNwsITFV4ANSY5NcihwKrB9yj1J0sw4KE5DVdWuJC8FPgkcAlxYVddPua1Z4uk9rWb+fU5AqmraPUiSVrmD5TSUJGmKDAtJUpdhoX3yMStajZJcmOTWJNdNu5dZYVhor3zMilax9wAnT7uJWWJYaF98zIpWpar6HHDbtPuYJYaF9mWpx6ysnVIvkqbIsJAkdRkW2hcfsyIJMCy0bz5mRRJgWGgfqmoXsPsxKzcCl/iYFa0GSS4GvgQ8NMmOJKdPu6e7Ox/3IUnq8shCktRlWEiSugwLSVKXYSFJ6jIsJEldhoU0oiS/TnJNkuuSfDDJffcx90VJ3rGf+9+Y5O1t+cQkjz3QnqWVYlhIo/t5VT2iqh4G3Am8ZKV2nGRNVc1X1ctb6UTAsNCqYVhIy/N54CFJHpTkY0muTXJlkofvOTHJ05JcleRrST6V5KhWPzvJe5N8EXhvO5r4RJL1DILoVe1I5s+SfCvJPdt2DxhelybBsJD2U5I1DL7j4+vAPwJfq6qHA68Hti2xyReAE6rqkQwe8/6aobHjgCdU1XN3F6rq28C/Aue0I5nPA58FntKmnAp8pKp+tZK/l7Qva6bdgHQQuU+Sa9ry54ELgKuAvwKoqk8nOSLJA/bYbh3wgSRHA4cC3xoa215VPx/hvc9nEDIfA04D/nq5v4S0HIaFNLqfV9UjhgtJRtnuPOBtVbU9yYnA2UNjPx1lB1X1xSTr2/aHVJVfJ6qJ8jSUdGA+DzwPBncwAT+oqh/vMeeB3PVo980j7vcnwP33qG0D/g1493IalQ6EYSEdmLOBRye5FngTS4fB2cAHk1wN/GDE/X4c+MvdF7hb7X3A4cDFB9SxtAw+dVY6SCR5FrCpql4w7V40e7xmIR0EkpzH4A6sU6bdi2aTRxaSpC6vWUiSugwLSVKXYSFJ6jIsJEldhoUkqev/AcncPsYSbZLGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the countplot implies that the training dataset is not imbalanced \n",
    "sns.countplot(df_train['Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Polarity', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the countplot implies that the testing dataset is not imbalanced \n",
    "sns.countplot(df_test['Polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract features out from the training and testing datasets\n",
    "# Transformed them to datasets for future procedure\n",
    "X_train = df_train['Sentence'].to_frame()\n",
    "X_test = df_test['Sentence'].to_frame()\n",
    "\n",
    "## To extract labels out from the training and testing datasets\n",
    "y_train = df_train['Polarity']\n",
    "y_test = df_test['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the dataset has only one feature, which is text, I mainly tested to use some of the NLP techiniques to transform this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build a process function to transform the text feature, including make all words lowercase, as well as remove URL, punctuation, numbers and lemmatize words.\n",
    "\n",
    "#After testing a few machine learning models, it was observed that remove stopwords will not imporve model performances, thus stopwords were not removed on purpose.\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def my_preprocess(doc):\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    doc = re.sub(r'http\\S+', '', doc)\n",
    "    \n",
    "    #Remove punctuation\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
    "    \n",
    "    #Remove numbers \n",
    "    doc = re.sub(r'\\d+', '', doc)\n",
    "    \n",
    "    # lemmatize\n",
    "    doc = [lemmer.lemmatize(w) for w in doc.split()]\n",
    "    \n",
    "    return ' '.join(doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To set up functions to perform TF-IDF on the text feature\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=my_preprocess,\n",
    "                             max_df = 0.9, \n",
    "                             min_df=0.01, \n",
    "                             max_features = 1000, \n",
    "                             ngram_range=[1,3])\n",
    "text_feature = 'Sentence'\n",
    "drop_features = ['Sentence']\n",
    "\n",
    "def fit_features(df):\n",
    "  global dtm\n",
    "  dtm = vectorizer.fit(df[text_feature])\n",
    "\n",
    "  return\n",
    "\n",
    "def transform_features(df):\n",
    "  global dtm\n",
    " \n",
    "  if dtm is None:\n",
    "    print(\"Need to call fit_features first!\")\n",
    "    return\n",
    "\n",
    "  dtm = vectorizer.transform(df[text_feature])\n",
    "\n",
    "  features = pd.DataFrame(dtm.toarray(), \n",
    "                      columns=['bow_{:s}'.format(name) for name in \n",
    "                               vectorizer.get_feature_names()], \n",
    "                      index=df.index)\n",
    "  \n",
    "  passthrough_features = [c for c in df.columns if c not in drop_features]\n",
    "  \n",
    "  features[passthrough_features] = df[passthrough_features]\n",
    "\n",
    "  features['len'] = df['Sentence'].apply(lambda x: len(x))\n",
    "  features['syllable_count'] = df['Sentence'].apply(\n",
    "        lambda x: textstat.syllable_count(x))\n",
    "  features['flesch_reading_ease'] = df['Sentence'].apply(\n",
    "      lambda x: textstat.flesch_reading_ease(x))\n",
    "  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the training text feature to fit the TF-IDF function,then use the fitted function to transfer the training text feature and testing feature.\n",
    "# Transformed training text features will be used to train the model, and transformed testing text features will be used to access model performance.\n",
    "\n",
    "fit_features(X_train)\n",
    "X_train_features = transform_features(X_train)\n",
    "X_test_features = transform_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.65557\n"
     ]
    }
   ],
   "source": [
    "# Tested decision tree, random forest, KNN and XGBoost models and it turns out XGBoost has the best proformance\n",
    "\n",
    "clf_xg = XGBClassifier(n_estimators=100, max_depth=3)\n",
    "\n",
    "clf_xg.fit(X_train_features, y_train)\n",
    "\n",
    "y_pred_xg = clf_xg.predict(X_test_features)\n",
    "\n",
    "print(\"AUC = {:.5f}\".format(roc_auc_score(y_test, y_pred_xg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 2min 26s, sys: 35.3 s, total: 2h 3min 1s\n",
      "Wall time: 33min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None,\n",
       "                                     validate_parameters=False,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                         'gamma': [0.5, 1, 1.5, 2, 5], 'max_depth': [1, 10, 20],\n",
       "                         'min_child_weight': [1, 5, 10],\n",
       "                         'subsample': [0.6, 0.8, 1.0]},\n",
       "             return_train_score=True, scoring='roc_auc')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conduct Grid Sarch on XGBoost model to get hyperparameters that will build a better model\n",
    "xg = XGBClassifier()\n",
    "\n",
    "params = {'min_child_weight': [1, 5, 10],'gamma': [0.5, 1, 1.5, 2, 5],'subsample': [0.6, 0.8, 1.0],\n",
    "      'colsample_bytree': [0.6, 0.8, 1.0], 'max_depth': [3, 4, 5], 'max_depth': [1, 10, 20]}\n",
    "\n",
    "gridsearch = GridSearchCV(xg, params, scoring='roc_auc', cv=5, return_train_score=True)\n",
    "\n",
    "%time gridsearch.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1.0,\n",
       " 'gamma': 1,\n",
       " 'max_depth': 20,\n",
       " 'min_child_weight': 1,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.804922689866945"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=20,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1.0,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the best hypermeters from the hyperparameter tunning \n",
    "gridsearch.best_params_\n",
    "gridsearch.best_score_\n",
    "gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1.0, gamma=1, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=20,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.64410\n"
     ]
    }
   ],
   "source": [
    "#Re-run the XGBoost model with hyperparameters from the hypermeters tunning process \n",
    "#Compute the AUC score to compare with the previous model and it turns out that grid search hyperparameter tunning didn't improve model proformance\n",
    "\n",
    "clf_xg2 = XGBClassifier(colsample_bytree =1.0, gamma =1, max_depth=20, min_child_weight=1,subsample=1)\n",
    "\n",
    "\n",
    "clf_xg2.fit(X_train_features, y_train)\n",
    "\n",
    "y_pred_xg2 = clf_xg2.predict(X_test_features)\n",
    "\n",
    "\n",
    "print(\"AUC = {:.5f}\".format(roc_auc_score(y_test, y_pred_xg2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[214  73]\n",
      " [136 177]]\n",
      "\n",
      "F1 Micro Score = 0.65167\n",
      "\n",
      "F1 Macro Score = 0.65034\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67       287\n",
      "           1       0.71      0.57      0.63       313\n",
      "\n",
      "    accuracy                           0.65       600\n",
      "   macro avg       0.66      0.66      0.65       600\n",
      "weighted avg       0.66      0.65      0.65       600\n",
      "\n",
      "AUC = 0.65557\n"
     ]
    }
   ],
   "source": [
    "y_pred_xg = clf_xg.predict(X_test_features)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xg))\n",
    "\n",
    "print(\"\\nF1 Micro Score = {:.5f}\".format(f1_score(y_test, y_pred_xg, average=\"micro\")))\n",
    "print(\"\\nF1 Macro Score = {:.5f}\".format(f1_score(y_test, y_pred_xg, average=\"macro\")))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xg))\n",
    "\n",
    "print(\"AUC = {:.5f}\".format(roc_auc_score(y_test, y_pred_xg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Sentence_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the community it represents.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>it ha northern humour and positive about the community it represents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I rather enjoyed it.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i rather enjoyed it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I liked it.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i liked it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It really created a unique feeling though.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>it really created a unique feeling though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A world better than 95% of the garbage in the theatres today.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a world better than of the garbage in the theatre today</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Sentence  \\\n",
       "4   It has northern humour and positive about the community it represents.     \n",
       "5   I rather enjoyed it.                                                       \n",
       "6   I liked it.                                                                \n",
       "8   It really created a unique feeling though.                                 \n",
       "10  A world better than 95% of the garbage in the theatres today.              \n",
       "\n",
       "    Polarity  Predict  \\\n",
       "4   1         0         \n",
       "5   1         0         \n",
       "6   1         0         \n",
       "8   1         0         \n",
       "10  1         0         \n",
       "\n",
       "                                                      Sentence_processed  \n",
       "4   it ha northern humour and positive about the community it represents  \n",
       "5   i rather enjoyed it                                                   \n",
       "6   i liked it                                                            \n",
       "8   it really created a unique feeling though                             \n",
       "10  a world better than of the garbage in the theatre today               "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Five of the wrong predictions are listed below:\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_test['Predict'] = y_pred_xg\n",
    "df_test['Sentence_processed'] = df_test['Sentence'].apply(my_preprocess)\n",
    "df_test[df_test['Predict'] != df_test['Polarity']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Reasons of Incorrect Predictions\n",
    "\n",
    "The first five wrong predictions are listed above in line 15. Comparing with the first ten correct predictions, which are listed below in line 16, one of the patterns can be observed is that the incorrectly predicted instances are more likely to have a general 'Sentence\". For example, sentences of \"I liked it\" and \"I enjoyed it\" are like high-level summary, they are only stating whether or not the movie is good without much explanation. Differently, the correctly predicted instances are more likely to include some explanations in the sentences, such as the role was played well, or the scripts were written well. Having more explanations can give the machine learning model more inputs to assess and predict. \n",
    "\n",
    "The other reasons might be that the training dataset and testing dataset are having different set of commonly used words. As presented in line 18 and line 19, the top-ten frequently used words in training dataset are “good”, “great” “phone”, “food”, “place”, “service”, “time”, “work”, “one”, and “like”; whereas the top-ten frequently used words in testing dataset are “film”, “movie” “one”, “bad”, “like”, “character”, “good”, “time”, “acting”, and “great”. Five words appear in both training and testing datasets when looking at top-ten frequently used words. Moreover, no word appears in both testing and training datasets when looking at top-five frequently used words. This situation could lead to that the model is performing well on training dataset but not performing well on testing dataset.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting code and outputs for Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Sentence_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubtedly a film worth seeing.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a good commentary of today love and undoubtedly a film worth seeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making, I think they did an excellent job!!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>for people who are first timer in film making i think they did an excellent job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, a good house and very good reactions and plenty of laughs.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>it wa very popular when i wa in the cinema a good house and very good reaction and plenty of laugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt when I came out of the cinema!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>it a feelgood film and thats how i felt when i came out of the cinema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I couldn't take them seriously.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i couldnt take them seriously</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     Sentence  \\\n",
       "0  A good commentary of today's love and undoubtedly a film worth seeing.                                       \n",
       "1  For people who are first timers in film making, I think they did an excellent job!!                          \n",
       "2  It was very popular when I was in the cinema, a good house and very good reactions and plenty of laughs.     \n",
       "3  It's a feel-good film and that's how I felt when I came out of the cinema!                                   \n",
       "7  I couldn't take them seriously.                                                                              \n",
       "\n",
       "   Polarity  Predict  \\\n",
       "0  1         1         \n",
       "1  1         1         \n",
       "2  1         1         \n",
       "3  1         1         \n",
       "7  0         0         \n",
       "\n",
       "                                                                                   Sentence_processed  \n",
       "0  a good commentary of today love and undoubtedly a film worth seeing                                 \n",
       "1  for people who are first timer in film making i think they did an excellent job                     \n",
       "2  it wa very popular when i wa in the cinema a good house and very good reaction and plenty of laugh  \n",
       "3  it a feelgood film and thats how i felt when i came out of the cinema                               \n",
       "7  i couldnt take them seriously                                                                       "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test['Predict'] == df_test['Polarity']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To develop a new preprocess function that is similiar than the previous but add removing stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('spanish'))\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess2(x):\n",
    "    # Remove HTML tags\n",
    "    x = BeautifulSoup(x, \"lxml\").get_text()\n",
    "\n",
    "    # Lower case\n",
    "    x = x.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)\n",
    "    \n",
    "    # Remove non-unicode\n",
    "    x = unidecode.unidecode(x)\n",
    "    \n",
    "    # Remove numbers\n",
    "    x = re.sub(r'\\d+', '', x)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    x = [lemmer.lemmatize(w) for w in x.split() if w not in stop_words]\n",
    "    return ' '.join(x) \n",
    "\n",
    "train_sentence = df_train['Sentence'].apply(preprocess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.8 ms, sys: 5.71 ms, total: 70.5 ms\n",
      "Wall time: 69.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('good', 194),\n",
       " ('great', 181),\n",
       " ('phone', 174),\n",
       " ('food', 124),\n",
       " ('place', 117),\n",
       " ('service', 107),\n",
       " ('time', 99),\n",
       " ('work', 98),\n",
       " ('one', 95),\n",
       " ('like', 92)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To present the top ten 1-grams for the training dataset\n",
    "no_features = 1000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(min_df=.01, max_df=.9, max_features=no_features, ngram_range=[1,3])\n",
    "%time dtm_tf = tf_vectorizer.fit_transform(train_sentence)\n",
    "\n",
    "##Calculate column sums from DTM and sort them\n",
    "sum_words = dtm_tf.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in tf_vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "##Display top ten\n",
    "words_freq[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 ms, sys: 231 µs, total: 20.8 ms\n",
      "Wall time: 20.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('film', 124),\n",
       " ('movie', 119),\n",
       " ('one', 52),\n",
       " ('bad', 47),\n",
       " ('like', 35),\n",
       " ('character', 34),\n",
       " ('good', 33),\n",
       " ('time', 33),\n",
       " ('acting', 30),\n",
       " ('great', 26)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To present the top ten 1-grams for the testing dataset\n",
    "\n",
    "test_sentence = df_test['Sentence'].apply(preprocess2)\n",
    "tf_vectorizer = CountVectorizer(min_df=.01, max_df=.9, max_features=no_features, ngram_range=[1,3])\n",
    "%time dtm_tf = tf_vectorizer.fit_transform(test_sentence)\n",
    "\n",
    "##Calculate column sums from DTM and sort them\n",
    "sum_words = dtm_tf.sum(axis=0) \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in tf_vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "## Display top ten\n",
    "words_freq[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
